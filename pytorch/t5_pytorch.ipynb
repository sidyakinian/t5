{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sWCbNC6QSZb"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO6ejCxnQPYe",
        "outputId": "1a84a8ea-93c7-4d5b-9b14-54be0f081133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jqFYzpoKQrLt"
      },
      "outputs": [],
      "source": [
        "from pdb import set_trace as bkpt\n",
        "import math\n",
        "from typing import Callable\n",
        "import gc\n",
        "from time import sleep\n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module, Embedding, Linear, LayerNorm, MultiheadAttention, ModuleList, Softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "from transformers import T5Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnYCvfrdQTRX"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cenKOZzbQakJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    d_model: int\n",
        "    d_ff: int\n",
        "    n_heads: int\n",
        "    d_k: int\n",
        "    num_relative_pos: int\n",
        "    eps: float = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7epcfJZGQdKB"
      },
      "outputs": [],
      "source": [
        "def relative_position_bucket(relative_position: Tensor, is_decoder: bool, num_buckets=32, max_distance=128) -> Tensor:\n",
        "        relative_buckets = 0\n",
        "        if is_decoder:\n",
        "            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n",
        "        else:\n",
        "            num_buckets //= 2\n",
        "            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
        "            relative_position = torch.abs(relative_position)\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_position < max_exact\n",
        "\n",
        "        relative_position_if_large = max_exact + (\n",
        "            torch.log(relative_position.float() / max_exact)\n",
        "            / math.log(max_distance / max_exact)\n",
        "            * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_position_if_large = torch.min(\n",
        "            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "def compute_bias(query_length: int, key_length: int, embedding: Callable, is_decoder: bool, device=None) -> Tensor:\n",
        "    if device is None:\n",
        "        device = embedding.pos_embedding_layer.weight.device\n",
        "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n",
        "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
        "    relative_position = memory_position - context_position  # shape (query_length, key_length)\n",
        "    position_bucket = relative_position_bucket(\n",
        "        relative_position,  # shape (query_length, key_length)\n",
        "        is_decoder=is_decoder,\n",
        "    )\n",
        "    values = embedding(position_bucket)  # shape (query_length, key_length, n_heads)\n",
        "    values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, n_heads, query_length, key_length)\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "896DRwxpQeTc"
      },
      "outputs": [],
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.word_embedding_layer = Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.word_embedding_layer(x)\n",
        "\n",
        "\n",
        "class PositionEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(PositionEmbeddingLayer, self).__init__()\n",
        "        self.pos_embedding_layer = Embedding(config.num_relative_pos, config.n_heads)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.pos_embedding_layer(x)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(MLP, self).__init__()\n",
        "        self.ff1 = Linear(config.d_model, config.d_ff, bias=False)\n",
        "        self.ff2 = Linear(config.d_ff, config.d_model, bias=False)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.activation(self.ff1(x))\n",
        "        return self.ff2(x)\n",
        "\n",
        "\n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(config.d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        variance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
        "        x = x * torch.rsqrt(variance + self.eps)\n",
        "\n",
        "        # convert into half-precision if necessary\n",
        "        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n",
        "            hidden_states = hidden_states.to(self.weight.dtype)\n",
        "\n",
        "        return self.weight * x\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig, is_decoder: bool):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_k = int(config.d_model / config.n_heads)\n",
        "        self.w_q = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_k = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_v = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_o = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "    def split_heads(self, x: Tensor) -> Tensor:\n",
        "        batch_size, n, _ = x.size()\n",
        "        x = x.view((batch_size, n, self.n_heads, self.d_k))\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def unify_heads(self, x: Tensor) -> Tensor:\n",
        "        batch_size, _, n, _ = x.size()\n",
        "        x = x.transpose(1, 2)\n",
        "        x = x.reshape((batch_size, n, self.d_model))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor, position_bias: Tensor) -> Tensor:\n",
        "        _, n, _ = x.size()\n",
        "        if self.is_decoder:\n",
        "            Q = self.w_q(x[:, -1, :].unsqueeze(1))\n",
        "        else:\n",
        "            Q = self.w_q(x)\n",
        "        K, V = self.w_k(x), self.w_v(x)\n",
        "        Q, K, V = self.split_heads(Q), self.split_heads(K), self.split_heads(V)\n",
        "        scores = (Q @ K.transpose(-1, -2))\n",
        "        scores += position_bias\n",
        "        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
        "        split_attention = attn_weights @ V\n",
        "        attention = self.unify_heads(split_attention)\n",
        "        output = self.w_o(attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class EncDecAttention(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(EncDecAttention, self).__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_k = int(config.d_model / config.n_heads)\n",
        "        self.w_q = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_k = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_v = Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_o = Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def split_heads(self, x: Tensor) -> Tensor:\n",
        "        batch_size, n, _ = x.size()\n",
        "        x = x.view((batch_size, n, self.n_heads, self.d_k))\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def unify_heads(self, x: Tensor) -> Tensor:\n",
        "        batch_size, _, n, _ = x.size()\n",
        "        x = x.transpose(1, 2)\n",
        "        x = x.reshape((batch_size, n, self.d_model))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor, encoding: Tensor, position_bias: Tensor) -> Tensor:\n",
        "        _, n, _ = x.size()\n",
        "        _, encoding_n, _ = encoding.size()\n",
        "        # Q = self.w_q(x[:, -1, :].unsqueeze(1))\n",
        "        Q = self.w_q(x) # not sure about cross-attn implementation here\n",
        "        K, V = self.w_k(encoding), self.w_v(encoding)\n",
        "        Q, K, V = self.split_heads(Q), self.split_heads(K), self.split_heads(V)\n",
        "        scores = (Q @ K.transpose(-1, -2))\n",
        "        scores += position_bias\n",
        "        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
        "        split_attention = attn_weights @ V\n",
        "        attention = self.unify_heads(split_attention)\n",
        "        output = self.w_o(attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):    \n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = SelfAttention(config=config, is_decoder=False)\n",
        "        self.norm1 = Norm(config)\n",
        "        self.mlp = MLP(config=config)\n",
        "        self.norm2 = Norm(config)\n",
        "\n",
        "    def forward(self, x: Tensor, position_bias: Tensor) -> Tensor:\n",
        "        normed_x = self.norm1(x)\n",
        "        x = x + self.self_attention(normed_x, position_bias=position_bias)\n",
        "        normed_x = self.norm2(x)\n",
        "        x = x + self.mlp(normed_x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = SelfAttention(config=config, is_decoder=True)\n",
        "        self.norm1 = Norm(config)\n",
        "        self.enc_dec_attention = EncDecAttention(config=config)\n",
        "        self.norm2 = Norm(config)\n",
        "        self.mlp = MLP(config=config)\n",
        "        self.norm3 = Norm(config)\n",
        "\n",
        "    def forward(self, x: Tensor, encoding: Tensor, self_attention_position_bias: Tensor, enc_dec_attention_position_bias: Tensor) -> Tensor:\n",
        "        normed_x = self.norm1(x)\n",
        "        attn_output = self.self_attention(normed_x, position_bias=self_attention_position_bias)\n",
        "        x += attn_output\n",
        "        normed_x = self.norm2(x)\n",
        "        cross_attn_output = self.enc_dec_attention(normed_x, encoding, position_bias=enc_dec_attention_position_bias)\n",
        "        x += cross_attn_output\n",
        "        normed_x = self.norm3(x)\n",
        "        mlp_output = self.mlp(normed_x)\n",
        "        x += mlp_output\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = ModuleList([EncoderLayer(config=config) for i in range(config.n_layers)])\n",
        "        self.self_attention_relative_attention_embedding = PositionEmbeddingLayer(config)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        _, n, _ = x.size()\n",
        "        self.self_attention_position_bias = compute_bias(n, n, self.self_attention_relative_attention_embedding, is_decoder=False)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, self.self_attention_position_bias)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = ModuleList([DecoderLayer(config=config) for i in range(config.n_layers)])\n",
        "        self.self_attention_relative_attention_embedding = PositionEmbeddingLayer(config)\n",
        "        self.enc_dec_attention_relative_attention_embedding = PositionEmbeddingLayer(config)\n",
        "\n",
        "    def forward(self, x: Tensor, encoding: Tensor) -> Tensor:\n",
        "        _, n, _ = x.size()\n",
        "        _, encoding_n, _ = encoding.size()\n",
        "        self_attention_position_bias = compute_bias(1, n, self.self_attention_relative_attention_embedding, is_decoder=True)\n",
        "        enc_dec_attention_position_bias = compute_bias(1, encoding_n, self.enc_dec_attention_relative_attention_embedding, is_decoder=True)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, \n",
        "                      encoding, \n",
        "                      self_attention_position_bias=self_attention_position_bias, \n",
        "                      enc_dec_attention_position_bias=enc_dec_attention_position_bias)\n",
        "        return x[:, -1, :].unsqueeze(1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding_layer = EmbeddingLayer(config=config)\n",
        "        self.encoder = Encoder(config=config)\n",
        "        self.final_encoder_layer_norm = Norm(config=config)\n",
        "        self.decoder = Decoder(config=config)\n",
        "        self.final_decoder_layer_norm = Norm(config=config)\n",
        "        self.d_model = config.d_model\n",
        "        self.lm_head = Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: Tensor, max_tokens=50, debug=False) -> Tensor:\n",
        "        x_emb = self.embedding_layer(x)\n",
        "        encoding = self.encoder(x_emb)\n",
        "        encoding = self.final_encoder_layer_norm(encoding)\n",
        "        outputs = torch.IntTensor([[0]]).to(\"cuda\")\n",
        "        while outputs[:, -1] != 1 and len(outputs[0]) < max_tokens:\n",
        "            if debug:\n",
        "                bkpt()\n",
        "            decoder_inputs_embedding = self.embedding_layer(outputs)\n",
        "            decoding = self.decoder(decoder_inputs_embedding, encoding)\n",
        "            decoding = self.final_decoder_layer_norm(decoding)\n",
        "            decoding *= (self.d_model ** -0.5)\n",
        "            next_logits = self.lm_head(decoding)\n",
        "            next_token = torch.argmax(next_logits, dim=-1)\n",
        "            outputs = torch.cat([outputs, next_token], dim=-1)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W62BOcKhQVON"
      },
      "source": [
        "### Initialize and load weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sUQv8q63Qixf"
      },
      "outputs": [],
      "source": [
        "transformer_config = TransformerConfig(vocab_size=32128,\n",
        "                                       n_layers=6,\n",
        "                                       d_model=512,\n",
        "                                       d_ff=2048,\n",
        "                                       n_heads=8,\n",
        "                                       d_k=64,\n",
        "                                       num_relative_pos=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SpbM_VBnQk3E"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(config=transformer_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7TFeUGlShRB",
        "outputId": "d020c85c-6274-4a38-cf14-0373491396f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S6M5yJCDQluk"
      },
      "outputs": [],
      "source": [
        "# Upload your weights here!\n",
        "# Download from https://huggingface.co/t5-small/tree/main and upload to your Google Drive in the root folder\n",
        "t5_small_weights = torch.load(\"drive/MyDrive/t5-small-torch-weights.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qd6V6k9rRAGH"
      },
      "outputs": [],
      "source": [
        "def dict_from_array_of_dicts(dicts_array):\n",
        "    res = {}\n",
        "    for dict_element in dicts_array:\n",
        "        res = {**res, **dict_element}\n",
        "    return res\n",
        "\n",
        "embedding_dict = {'embedding_layer.word_embedding_layer.weight': torch.Size([32128, 512])}\n",
        "first_encoder_layer_dict = {\n",
        " 'encoder.layers.0.self_attention.w_q.weight': torch.Size([512, 512]),\n",
        " 'encoder.layers.0.self_attention.w_k.weight': torch.Size([512, 512]),\n",
        " 'encoder.layers.0.self_attention.w_v.weight': torch.Size([512, 512]),\n",
        " 'encoder.layers.0.self_attention.w_o.weight': torch.Size([512, 512]),\n",
        " 'encoder.self_attention_relative_attention_embedding.pos_embedding_layer.weight': torch.Size([32, 8]),\n",
        " 'encoder.layers.0.norm1.weight': torch.Size([512]),\n",
        " 'encoder.layers.0.mlp.ff1.weight': torch.Size([2048, 512]),\n",
        " 'encoder.layers.0.mlp.ff2.weight': torch.Size([512, 2048]),\n",
        " 'encoder.layers.0.norm2.weight': torch.Size([512])\n",
        "}\n",
        "encoder_dicts = [{\n",
        "    f'encoder.layers.{i}.self_attention.w_q.weight': torch.Size([512, 512]),\n",
        "    f'encoder.layers.{i}.self_attention.w_k.weight': torch.Size([512, 512]),\n",
        "    f'encoder.layers.{i}.self_attention.w_v.weight': torch.Size([512, 512]),\n",
        "    f'encoder.layers.{i}.self_attention.w_o.weight': torch.Size([512, 512]),\n",
        "    f'encoder.layers.{i}.norm1.weight': torch.Size([512]),\n",
        "    f'encoder.layers.{i}.mlp.ff1.weight': torch.Size([2048, 512]),\n",
        "    f'encoder.layers.{i}.mlp.ff2.weight': torch.Size([512, 2048]),\n",
        "    f'encoder.layers.{i}.norm2.weight': torch.Size([512]),\n",
        "} for i in range(1, 6)]\n",
        "final_encoder_layer_norm_dict = {'final_encoder_layer_norm.weight': torch.Size([512])}\n",
        "first_decoder_layer_dict = {\n",
        " 'decoder.layers.0.self_attention.w_q.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.self_attention.w_k.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.self_attention.w_v.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.self_attention.w_o.weight': torch.Size([512, 512]),\n",
        " 'decoder.self_attention_relative_attention_embedding.pos_embedding_layer.weight': torch.Size([32, 8]),\n",
        " 'decoder.layers.0.norm1.weight': torch.Size([512]),\n",
        " 'decoder.layers.0.enc_dec_attention.w_q.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.enc_dec_attention.w_k.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.enc_dec_attention.w_v.weight': torch.Size([512, 512]),\n",
        " 'decoder.layers.0.enc_dec_attention.w_o.weight': torch.Size([512, 512]),\n",
        " 'decoder.enc_dec_attention_relative_attention_embedding.pos_embedding_layer.weight': torch.Size([32, 8]),\n",
        " 'decoder.layers.0.norm2.weight': torch.Size([512]),\n",
        " 'decoder.layers.0.mlp.ff1.weight': torch.Size([2048, 512]),\n",
        " 'decoder.layers.0.mlp.ff2.weight': torch.Size([512, 2048]),\n",
        " 'decoder.layers.0.norm3.weight': torch.Size([512]),\n",
        "}\n",
        "decoder_dicts = [{\n",
        "    f'decoder.layers.{i}.self_attention.w_q.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.self_attention.w_k.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.self_attention.w_v.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.self_attention.w_o.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.norm1.weight': torch.Size([512]),\n",
        "    f'decoder.layers.{i}.enc_dec_attention.w_q.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.enc_dec_attention.w_k.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.enc_dec_attention.w_v.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.enc_dec_attention.w_o.weight': torch.Size([512, 512]),\n",
        "    f'decoder.layers.{i}.norm2.weight': torch.Size([512]),\n",
        "    f'decoder.layers.{i}.mlp.ff1.weight': torch.Size([2048, 512]),\n",
        "    f'decoder.layers.{i}.mlp.ff2.weight': torch.Size([512, 2048]),\n",
        "    f'decoder.layers.{i}.norm3.weight': torch.Size([512]),\n",
        "} for i in range(1, 6)]\n",
        "final_decoder_layer_norm_dict = {'final_decoder_layer_norm.weight': torch.Size([512])}\n",
        "lm_head_dict = {'lm_head.weight': torch.Size([32128, 512])}\n",
        "\n",
        "new_weights = {\n",
        "    **embedding_dict,\n",
        "    **first_encoder_layer_dict,\n",
        "    **(dict_from_array_of_dicts(encoder_dicts)),\n",
        "    **final_encoder_layer_norm_dict,\n",
        "    **first_decoder_layer_dict,\n",
        "    **(dict_from_array_of_dicts(decoder_dicts)),\n",
        "    **final_decoder_layer_norm_dict,\n",
        "    **lm_head_dict\n",
        "}\n",
        "\n",
        "t5_small_weights['lm_head.weight'] = t5_small_weights['shared.weight']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Aeu8aErRIv4",
        "outputId": "fcf97d6d-c37f-4311-9340-980388d1d284"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(133, 133)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check that number of parameter tensors matches\n",
        "len(new_weights.keys()), len(t5_small_weights.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "blxqo7F9RL8c"
      },
      "outputs": [],
      "source": [
        "# Copy weights\n",
        "t5_modified_weights = OrderedDict()\n",
        "\n",
        "for (kv0, kv1) in zip(t5_small_weights.items(), new_weights.items()):\n",
        "    key0, value0 = kv0\n",
        "    key1, value1 = kv1\n",
        "    t5_modified_weights[key1] = value0.detach().clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mHhv6XKZRWXn"
      },
      "outputs": [],
      "source": [
        "# Call this function to load new weights\n",
        "def load_weights():\n",
        "    transformer.load_state_dict(t5_modified_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0OICO8RIRaUs"
      },
      "outputs": [],
      "source": [
        "load_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B2Ub2h8nRsp4"
      },
      "outputs": [],
      "source": [
        "# Put model on GPU\n",
        "transformer = transformer.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhJgd_MBQZkJ"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "6d9682730b144619a9d2a9ac8e28b270",
            "7ee324f6bc40419297e0a48c7b281953",
            "8915dd8dce6944399e2decb81b95b0b4",
            "9a86b123389f49548d0ea4d1c69f15e4",
            "99291786754240ce934e0e411974d297",
            "e579996ac3aa4f52b8910efc0d0f5ea9",
            "54101ef765a04eef9482dd311b9e3c12",
            "56a3e21a81c24630a2931aa3c45d5a6c",
            "6f9ff2cf53fa4a1d8fe636a4fc394008",
            "a60ecd57275e4b4bbbef1ada36cbed8c",
            "925a6df6f12b4ad7b6d1c1b9698c6ff4",
            "a37975dca7b8430faccab7d1c8aab66e",
            "353997b565b548d4b2bfbb65d1ecd3f2",
            "bbf83607c6eb4837982db80656e21617",
            "05e5d378a466412588a698d140f7eae3",
            "c82f0ce1e1724b52b057807e229eb7f9",
            "2bd637b974734766aea56245b8254659",
            "e744ec7fdf9b42298a5003c50a1ed97b",
            "9848bbbc0fe9496eb9a784653bdaa0c8",
            "1910b667ccdc41baa409308a55c99554",
            "c0af453c2a714bbfae403e25e5454b91",
            "1d66dc12b2c94f9d8a55329eafdf6742"
          ]
        },
        "id": "QR4brqOSRwaj",
        "outputId": "e54551f4-1331-4382-de7a-f918b363a787"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9682730b144619a9d2a9ac8e28b270",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a37975dca7b8430faccab7d1c8aab66e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Use the tokenizer from Hugging Face transformers library\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXrwJ2JRR0Xo",
        "outputId": "e6ee3f51-24ce-43d6-d42f-84fd98d05be4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[21603,    10,  5844,  9740,    65,   373, 24631,  6917,     6,    11,\n",
              "            65,  2237,    12,     3, 11394, 25175,    11, 14500,     7,    16,\n",
              "           748,     5,    37,   810,    13,    69,  3693,   358,    11,  1909,\n",
              "          3256,    12, 19019,   126, 29063,     6,    45,     8,  3239,    13,\n",
              "          4345,     7,    12,     8,   960,    21,   996,   449,  6216, 12042,\n",
              "           280,     5,   438,     8,   199,    13,  2496,  2896,   224,    38,\n",
              "         27480,     7,    11,   628,  6696,     6,  7004,    33,     3,   179,\n",
              "            12,  2485,     3,     9,  7231,  1705,    13,     8,  8084,    11,\n",
              "            69,   286,    16,    34,     5,    37, 13709,    13,  1103,    11,\n",
              "          9087,  9350,  2925,  1729,    16,   628,  9740,     6,  1374,    12,\n",
              "             3,     9,   394,  1705,    13,    59,   131,     8,   576,     7,\n",
              "          3972,     6,    68,    92,  3242,     5,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       device='cuda:0')}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"summarize: Space exploration has always fascinated humans, and has led to countless discoveries and advancements in technology. The study of our solar system and beyond continues to uncover new mysteries, from the formation of planets to the search for extraterrestrial life. With the help of advanced technologies such as telescopes and spacecraft, scientists are able to gain a deeper understanding of the universe and our place in it. The pursuit of knowledge and discovery drives continued investment in space exploration, leading to a better understanding of not just the cosmos, but also ourselves.\"\n",
        "inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
        "inputs.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WNrrfp6RR3Kt"
      },
      "outputs": [],
      "source": [
        "use_profiler = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gmWxYT_oR4C9",
        "outputId": "2997de1d-0f61-4330-e499-0563f16d5c92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the study of our solar system and beyond continues to uncover new mysteries.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if use_profiler:\n",
        "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "        with record_function(\"model_inference\"):\n",
        "            transformer_outputs = transformer(inputs.input_ids, debug=False, max_tokens=20)\n",
        "else:\n",
        "    transformer_outputs = transformer(inputs.input_ids, debug=False, max_tokens=50)\n",
        "    \n",
        "decoded_outputs = tokenizer.decode(transformer_outputs[0], skip_special_tokens=True)\n",
        "decoded_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drBz3STfSPht"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05e5d378a466412588a698d140f7eae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0af453c2a714bbfae403e25e5454b91",
            "placeholder": "​",
            "style": "IPY_MODEL_1d66dc12b2c94f9d8a55329eafdf6742",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 39.8kB/s]"
          }
        },
        "1910b667ccdc41baa409308a55c99554": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d66dc12b2c94f9d8a55329eafdf6742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd637b974734766aea56245b8254659": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353997b565b548d4b2bfbb65d1ecd3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd637b974734766aea56245b8254659",
            "placeholder": "​",
            "style": "IPY_MODEL_e744ec7fdf9b42298a5003c50a1ed97b",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "54101ef765a04eef9482dd311b9e3c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56a3e21a81c24630a2931aa3c45d5a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d9682730b144619a9d2a9ac8e28b270": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ee324f6bc40419297e0a48c7b281953",
              "IPY_MODEL_8915dd8dce6944399e2decb81b95b0b4",
              "IPY_MODEL_9a86b123389f49548d0ea4d1c69f15e4"
            ],
            "layout": "IPY_MODEL_99291786754240ce934e0e411974d297"
          }
        },
        "6f9ff2cf53fa4a1d8fe636a4fc394008": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ee324f6bc40419297e0a48c7b281953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e579996ac3aa4f52b8910efc0d0f5ea9",
            "placeholder": "​",
            "style": "IPY_MODEL_54101ef765a04eef9482dd311b9e3c12",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "8915dd8dce6944399e2decb81b95b0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a3e21a81c24630a2931aa3c45d5a6c",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f9ff2cf53fa4a1d8fe636a4fc394008",
            "value": 791656
          }
        },
        "925a6df6f12b4ad7b6d1c1b9698c6ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9848bbbc0fe9496eb9a784653bdaa0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99291786754240ce934e0e411974d297": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a86b123389f49548d0ea4d1c69f15e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a60ecd57275e4b4bbbef1ada36cbed8c",
            "placeholder": "​",
            "style": "IPY_MODEL_925a6df6f12b4ad7b6d1c1b9698c6ff4",
            "value": " 792k/792k [00:00&lt;00:00, 1.87MB/s]"
          }
        },
        "a37975dca7b8430faccab7d1c8aab66e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_353997b565b548d4b2bfbb65d1ecd3f2",
              "IPY_MODEL_bbf83607c6eb4837982db80656e21617",
              "IPY_MODEL_05e5d378a466412588a698d140f7eae3"
            ],
            "layout": "IPY_MODEL_c82f0ce1e1724b52b057807e229eb7f9"
          }
        },
        "a60ecd57275e4b4bbbef1ada36cbed8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf83607c6eb4837982db80656e21617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9848bbbc0fe9496eb9a784653bdaa0c8",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1910b667ccdc41baa409308a55c99554",
            "value": 1206
          }
        },
        "c0af453c2a714bbfae403e25e5454b91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82f0ce1e1724b52b057807e229eb7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e579996ac3aa4f52b8910efc0d0f5ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e744ec7fdf9b42298a5003c50a1ed97b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
