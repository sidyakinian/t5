![1*D0J1gNQf8vrrUpKeyD8wPA](https://user-images.githubusercontent.com/34050187/217142459-6b3e56d5-16e9-458e-a4a8-c4ed1114626f.png)

# Implementation of T5 transformer model in PyTorch and JAX

The T5 transformer model is a natural language processing model developed by Google. It uses the transformer architecture, which has been widely adopted in NLP for processing and understanding long sequences of text data. T5 is capable of performing a variety of NLP tasks, including text classification, machine translation, and summarization. The model is trained on a large dataset, which helps it produce coherent text and perform NLP tasks with a high level of accuracy.

This repository implements T5 using two deep learning frameworks: PyTorch and JAX. The goal of this project is to provide a simple example of a working transformer implementation.

### How to run the models

1. Navigate to `pytorch` or `jax` folder and open the T5 notebook you'd like to run.
2. In your browser address bar, change `github.com` in address to `githubtocolab.com`.
3. Make sure to download the weights and upload them to your Google Drive.
4. Run the notebook!

### Credits

* [Useful article](https://e2eml.school/transformers.html#rest_stop) for understanding transformers.
* HuggingFace implementations of T5 have been very helpful.
